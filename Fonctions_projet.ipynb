{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions pour projet\n",
    "\n",
    "need to normalize data ? (cf Dissecting financial markets: Sectors and states) (on devrait utiliser les returns donc pas trop besoin de normaliser)\n",
    "- si on cluster les assets, logiquement la correlation/covariance entre chaque cluster devrait être négative/nulle (parce que la distance serait sans doute la covariance entre les assets), du coup faire une stratégie qui investit dans un représentant (asset ou weigted average) de chaque cluster devrait réduire le risque/volatilité du portfolio (problème étant que on aurait pas un zero-cost portfolio), ou alors prédire le signe de return de chaque cluster à la prochaine période et donc investir en fonction)\n",
    "- si on cluster par jour/période de temps, l'idée serait de prédire le futur état et pour chaque état définir une méthode d'investissement\n",
    "- https://d-nb.info/1108447864/34 papier qui parle de cluster les assets: truc important à noter est que les weights pour chaque cluster sont définis par hierarchical clustering p47-48, et les clusters/weights sont recalculés tous les 130 jours (permet d'éviter de refaire les clusters à chaque fois, et en pls ne suppose plus que les clusters restent les memes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Pour faire des clusters\n",
    "#ce qui doit etre cluster sont les colonnes du dataframe (donc si on veut cluster les dates il faut transposer la matrice)\n",
    "\n",
    "\n",
    "from numpy import linalg as LA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import networkx as nx\n",
    "import community\n",
    "import dask\n",
    "dask.config.set(scheduler='processes')\n",
    "import glob\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Idea is to remove low eigenvalues that are considered as noise\n",
    "def compute_C_minus_C0(lambdas,v,lambda_plus,removeMarketMode=True): \n",
    "    N=len(lambdas)\n",
    "    C_clean=np.zeros((N, N))\n",
    "    \n",
    "    order = np.argsort(lambdas)\n",
    "    lambdas,v = lambdas[order],v[:,order]\n",
    "    \n",
    "    v_m=np.matrix(v)\n",
    "\n",
    "    # note that the eivenvalues are sorted\n",
    "    for i in range(1*removeMarketMode,N):                            \n",
    "        if lambdas[i]>lambda_plus: \n",
    "            C_clean=C_clean+lambdas[i] * np.dot(v_m[:,i],v_m[:,i].T)  \n",
    "    return C_clean    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LouvainCorrelationClustering(R):   # R is a matrix of return\n",
    "    N=R.shape[1]\n",
    "    T=R.shape[0]\n",
    "\n",
    "    q=N*1./T\n",
    "    lambda_plus=(1.+np.sqrt(q))**2\n",
    "\n",
    "    C=R.corr()\n",
    "    lambdas, v = LA.eigh(C)\n",
    "\n",
    "\n",
    "            \n",
    "    C_s=compute_C_minus_C0(lambdas,v,lambda_plus)\n",
    "    \n",
    "    mygraph= nx.from_numpy_matrix(np.abs(C_s))\n",
    "    partition = community.community_louvain.best_partition(mygraph,random_state=29)\n",
    "\n",
    "    DF=pd.DataFrame.from_dict(partition,orient=\"index\",columns=['Cluster'])\n",
    "    DF.index=R.columns\n",
    "    return(DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "peut etre qu'on peut faire une analyse des eigenvalues et eigenvectors (cf cours 9-10)\n",
    "mais la question est qu'on peut pas faire les eigenvalues sur tout le dataset (parce que dans ce cas on analyserait le futur avant meme de faire des clusters par exemple...)\n",
    "\n",
    "De plus, cf cours 11 on pourrait faire varier les méthodes de cluster, que ce soit en changeant la facon de faire le cluster ou de changer le critère qui doit être cluster (return, sign of return, excess return ...)\n",
    "\n",
    "et faudrait aussi du coup faire des investment strategies et les comparer entre elles, et avec un benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "import os\n",
    "from fastparquet import write, ParquetFile\n",
    "\n",
    "#Check if we already have the clustering or not and compute louvain cluster\n",
    "@dask.delayed\n",
    "def get_Louvain_cluster(R,filename,t_0,t_1, day_state=False) : \n",
    "    \n",
    "    # t_0 and t_1 = window\n",
    "    #day_state = TRUE -> market state clustering\n",
    "    \n",
    "    #Check if cluster already exists\n",
    "    if(exists(filename) and os.path.getsize(filename)>0):\n",
    "        \n",
    "        #Import clustering\n",
    "        DF=pd.read_parquet(filename)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        #Do clustering method and saves it\n",
    "        rolling_data=R.iloc[(R.index>=t_0) & (R.index<=t_1)]\n",
    "        \n",
    "        if day_state:\n",
    "            rolling_data=rolling_data.T\n",
    "            \n",
    "        DF= LouvainCorrelationClustering(rolling_data)\n",
    "        DF.to_parquet(filename)\n",
    "        \n",
    "    return(DF)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def RolledCluster(R, cluster_period, filepath, day_state=False, keep_all_info=False, lag_max=20):\n",
    "    \n",
    "    #R = dataframe\n",
    "    #cluster_period = length of rolling window\n",
    "    #day_state = TRUE -> market state clustering\n",
    "    #keep_all_info = TRUE -> rolling window expanding instead of just rolling (pas sur detre utile)\n",
    "    #lag_max = number of clustering\n",
    "    \n",
    "    \n",
    "    #Create list of clusters\n",
    "    liste=[]\n",
    "    \n",
    "    #\n",
    "    if keep_all_info: \n",
    "        if day_state :\n",
    "            for lag in range(1,lag_max):\n",
    "                \n",
    "                #initilisation of window\n",
    "                t_0=R.index[0]\n",
    "                t_1=R.index[0+lag+cluster_period]   \n",
    "                \n",
    "                #compute convenient filename\n",
    "                filename=filepath+\"/{}_{}_cluster_day_keep_all_info.parquet\".format(t_0,t_1)\n",
    "                \n",
    "                #compute cluster\n",
    "                DF= get_Louvain_cluster(R,filename,t_0,t_1, day_state)\n",
    "                \n",
    "                #add cluster to list\n",
    "                liste.append(DF)\n",
    "        else :\n",
    "            for lag in range(1,lag_max):  \n",
    "                \n",
    "                t_0=R.index[0]\n",
    "                t_1=R.index[0+lag+cluster_period]    \n",
    "                \n",
    "                filename=filepath+\"/{}_{}_cluster_keep_all_info.parquet\".format(t_0,t_1)\n",
    "        \n",
    "                DF= get_Louvain_cluster(R,filename,t_0,t_1,day_state)\n",
    "                liste.append(DF)\n",
    "    else : \n",
    "        if day_state :\n",
    "            for lag in range(1,lag_max):  \n",
    "                \n",
    "                t_0=R.index[0+lag]\n",
    "                t_1=R.index[0+lag+cluster_period] \n",
    "                \n",
    "                filename=filepath+\"/{}_{}_cluster_day.parquet\".format(t_0,t_1)\n",
    "                \n",
    "                DF= get_Louvain_cluster(R,filename,t_0,t_1,day_state)\n",
    "                liste.append(DF)\n",
    "        else :\n",
    "            for lag in range(1,lag_max):  \n",
    "                \n",
    "                t_0=R.index[0+lag]\n",
    "                t_1=R.index[0+lag+cluster_period]   \n",
    "                \n",
    "                filename=filepath+\"/{}_{}_cluster.parquet\".format(t_0,t_1)\n",
    "        \n",
    "                DF= get_Louvain_cluster(R,filename,t_0,t_1,day_state)\n",
    "                liste.append(DF)\n",
    "    return(liste)\n",
    "\n",
    "#Renvoie une liste de clusters\n",
    "#Choose the assets (thus N ), choose the calibration length \n",
    "#(T = N/3 is a good choice for market states, T = 3N is a good choice for asset classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the rolling ARI of the clusters\n",
    "from sklearn import metrics\n",
    "\n",
    "def RolledARI(liste):\n",
    "    \n",
    "    #Create list of ARI\n",
    "    ARI=[]\n",
    "    \n",
    "    \n",
    "    for element in range(0, len(liste)-1):\n",
    "        \n",
    "        #Compare consecutive clustering and add measure\n",
    "        new_df=pd.merge(liste[element], liste[element+1], left_index=True, right_index=True)\n",
    "        ARI.append(metrics.adjusted_rand_score(new_df[\"Cluster_x\"], new_df[\"Cluster_y\"]))\n",
    "        \n",
    "        \n",
    "    return(ARI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction qu'on avait fait mais pas sur que ca serve dans notre cas\n",
    "def compute_R(events,tau_max=1000,dtau=1):\n",
    "    taus=range(1,tau_max,dtau)\n",
    "    \n",
    "    R=[-(events[\"mid\"].diff(-tau)*events[\"s\"]).mean() for tau in taus]\n",
    "    \n",
    "    \n",
    "    return np.array(R)\n",
    "\n",
    "def compute_R_h(events):\n",
    "    R=pd.pivot_table(events.groupby(events.index.hour).apply(compute_R).apply(pd.Series), columns=events.groupby(events.index.hour).apply(compute_R).apply(pd.Series).index)\n",
    "    return (R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pour linstant renvoie simplement une matrice de similarite\n",
    "#l'entree (i,j) correspond au pourcentage d'éléments du cluster 2j qui appartiennent aussi au cluster 1i\n",
    "def keep_cluster_number(cluster1,cluster2):\n",
    "    \n",
    "    #rename clusters based on similarity\n",
    "    \n",
    "    #compute number of clusters within each clustering period\n",
    "    n=cluster1[\"Cluster\"].unique().max()\n",
    "    m=cluster2[\"Cluster\"].unique().max()\n",
    "    \n",
    "    #dummy constant to avoid changing twice clusters later\n",
    "    cst=10000\n",
    "    \n",
    "    #Create matrix of similarity\n",
    "    matrix = np.zeros((n+1,m+1))\n",
    "    \n",
    "    for i in range(n+1):\n",
    "        for j in range(m+1):\n",
    "            merged=pd.merge(cluster1,cluster2, left_index=True, right_index=True)\n",
    "            \n",
    "            #get number of days/ticker belonging to cluster x_i and y_j\n",
    "            num= len(merged[(merged[\"Cluster_y\"]==j) & (merged[\"Cluster_x\"]==i)])\n",
    "            \n",
    "            #divide by number of elements in cluster y_j\n",
    "            denom=len(merged[(merged[\"Cluster_y\"]==j)])\n",
    "            \n",
    "            matrix[i][j]= num/denom\n",
    "            #entry (i,j) is the % of elements in y_j coming from x_i\n",
    "    \n",
    "    #get a copy of cluster2 that we change\n",
    "    new_cluster2=cluster2.copy(deep=True) \n",
    "    \n",
    "    #inititialize lists of cluster values\n",
    "    used_values=[]\n",
    "    unrenamed_cluster=[]\n",
    "    \n",
    "    #array with number of element in each cluster\n",
    "    tmp=[]\n",
    "    for i in range(0,m+1):\n",
    "        tmp.append(len(cluster2[cluster2[\"Cluster\"]==i]))\n",
    "\n",
    "    #transform into df\n",
    "    dff=pd.DataFrame(data=tmp)\n",
    "\n",
    "    #sort values by ascending number of element\n",
    "    #relabel cluster based on most important (size) cluster\n",
    "    \n",
    "    dff=dff.sort_values(by=0,ascending=False)\n",
    "\n",
    "    for i in dff.index:\n",
    "        \n",
    "        #get the position of the highest similarity\n",
    "        value= np.argmax(matrix[:,i])\n",
    "    \n",
    "        #verify that this value is not already attributed to another cluster\n",
    "        if not np.isin(value,used_values):\n",
    "        \n",
    "            value_cluster=value\n",
    "        \n",
    "            #update the used values\n",
    "            used_values.append(value)\n",
    "        \n",
    "            #change the new clusters\n",
    "            #add dummy constant to avoid changing twice clusters\n",
    "            new_cluster2[new_cluster2[\"Cluster\"]==i]= (value_cluster+cst)\n",
    "        \n",
    "        else:\n",
    "            #update cluster that need to be allocated a number\n",
    "            unrenamed_cluster.append(i)\n",
    "\n",
    "    #all possible values for cluster number\n",
    "    possible_values= range(0,m+1)\n",
    "\n",
    "    for element in unrenamed_cluster:\n",
    "    \n",
    "        #get smallest unused value\n",
    "        value_cluster= np.min(np.where(~np.isin(possible_values,used_values)))\n",
    "    \n",
    "        #update used values\n",
    "        used_values.append(value)\n",
    "    \n",
    "        #change cluster number\n",
    "        new_cluster2[new_cluster2[\"Cluster\"]==element]= (value_cluster+cst)\n",
    "    \n",
    "    #substract cst to get actual clusters numbers\n",
    "    new_cluster2=new_cluster2-cst\n",
    "    \n",
    "    \n",
    "    return(new_cluster2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One time prediction of cluster belonging\n",
    "#lines = period t\n",
    "#col = period t+1\n",
    "\n",
    "def one_step_pred(cluster):\n",
    "    \n",
    "    #get number of clusters\n",
    "    m=cluster[\"Cluster\"].unique().max()\n",
    "    \n",
    "    #create square matrix\n",
    "    matrix = np.zeros((m+1,m+1))\n",
    "    \n",
    "    for p in range(len(cluster)-1):\n",
    "        i=cluster[\"Cluster\"].iloc[p]\n",
    "        tmp=p+1\n",
    "        j=cluster[\"Cluster\"].iloc[tmp]\n",
    "        \n",
    "        #adds 1 when you go from state i to state j\n",
    "        matrix[i][j]=matrix[i][j]+1\n",
    "        \n",
    "    #divide by total number of changes    \n",
    "    matrix=matrix/(len(cluster)-1)\n",
    "    \n",
    "    #matrix is now a probability matrix\n",
    "    #matrix(i,j) is the probability to go from state i to state j\n",
    "    \n",
    "    return(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pour run les fonctions et les tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#logret= pd.read_parquet(\"data/clean/1m/data_clean_1m.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lii= RolledCluster(logret,150,\"/Users/mrobaux/Desktop/Financial big data/projet\",True,False)\n",
    "\n",
    "#number_of_clusters=[]\n",
    "#for element in lii:\n",
    "#    number_of_clusters.append(element[\"Cluster\"].unique().max()+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Idee: faire que les numeros de cluster restent consistants au cours du temps\n",
    "\n",
    "#for i in range(len(lii)-1):\n",
    "#    lii[i+1]=keep_cluster_number(lii[i],lii[i+1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create sankey plot\n",
    "#!pip install plotly\n",
    "#Pour voir comment ca marche\n",
    "#Affiche un graphe de transition donc c'est plutot cool\n",
    "\n",
    "#import plotly.graph_objects as go\n",
    "#label = [\"Cluster 0\",\"Cluster 1\",\"Cluster 2\",\"Cluster 3\",\"Cluster 0\",\"Cluster 1\",\"Cluster 2\",\"Cluster 3\",\"Cluster 0\",\"Cluster 1\",\"Cluster 2\",\"Cluster 3\",\"Cluster 0\",\"Cluster 1\",\"Cluster 2\",\"Cluster 3\"]\n",
    "#source = [0,0,0,0,1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5,6,6,6,6,7,7,7,7,8,8,8,8]\n",
    "#target =[4,5,6,7,4,5,6,7,4,5,6,7,4,5,6,7,8,9,10,11,8,9,10,11,8,9,10,11,8,9,10,11]\n",
    "#value = [matrix[0][0],matrix[0][1],matrix[0][2],matrix[0][3],matrix[1][0],matrix[1][1],matrix[1][2],matrix[1][3],matrix[2][0],matrix[2][1],matrix[2][2],matrix[2][3],matrix[3][0],matrix[3][1],matrix[3][2],matrix[3][3],matrix1[0][0],matrix1[0][1],matrix1[0][2],matrix1[0][3],matrix1[1][0],matrix1[1][1],matrix1[1][2],matrix1[1][3],matrix1[2][0],matrix1[2][1],matrix1[2][2],matrix1[2][3],matrix1[3][0],matrix1[3][1],matrix1[3][2],matrix1[3][3]]\n",
    "#link=dict(source=source,target=target,value=value)\n",
    "#node= dict(label=label,pad=35, thickness=10)\n",
    "\n",
    "\n",
    "#data=go.Sankey(link=link, node=node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#go.Figure(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot d'un cluster vs un autre pour voir si on obtient qqc d'intéressant\n",
    "\n",
    "#test=lii[1]\n",
    "#index0= test[test[\"Cluster\"]==1].index\n",
    "#index1= test[test[\"Cluster\"]==2].index\n",
    "\n",
    "\n",
    "#mean0=logret.loc[index0].mean()\n",
    "#mean1=logret.loc[index1].mean()\n",
    "\n",
    "#plt.scatter(mean0,mean1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brouillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pydot\n",
    "#!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Supposer afficher un sankey hyper style mais ca veut pas mdr\n",
    "#states = [\"0\",\"1\",\"2\",\"3\"]\n",
    "#def _get_markov_edges(Q):\n",
    "#    edges = {}\n",
    "#    for col in Q.columns:\n",
    "#        for idx in Q.index:\n",
    "#            edges[(idx,col)] = Q.loc[idx,col]\n",
    "#    return edges\n",
    "\n",
    "#edges_wts = _get_markov_edges(prob_state_transitions)\n",
    "\n",
    "# create graph object\n",
    "#G = nx.MultiDiGraph()\n",
    "\n",
    "# nodes correspond to states\n",
    "#G.add_nodes_from(states)\n",
    "\n",
    "# edges represent transition probabilities\n",
    "#for k, v in edges_wts.items():\n",
    "#    tmp_origin, tmp_destination = k[0], k[1]\n",
    "#    G.add_edge(tmp_origin, tmp_destination, weight=v, label=v)\n",
    "#, prog='dot'\n",
    "#nx.drawing.nx_pydot.write_dot(G,path=os.environ[\"PATH\"])\n",
    "#pos = nx.drawing.nx_pydot.pydot_layout(G,prog=\"dot\")\n",
    "#nx.draw_networkx(G, pos)\n",
    "\n",
    "# create edge labels for jupyter plot but is not necessary\n",
    "#edge_labels = {(n1,n2):d['label'] for n1,n2,d in G.edges(data=True)}\n",
    "#nx.draw_networkx_edge_labels(G , pos, edge_labels=edge_labels)\n",
    "#nx.drawing.nx_pydot.write_dot(G, 'pet_dog_markov.dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Supposer afficher un plot sympa mais ca veut pas mdr\n",
    "\n",
    "\n",
    "#weigted_edges=[]\n",
    "#for i in range(4):\n",
    "#    for j in range(4):\n",
    "#        weigted_edges.append((i,j,prob_state_transitions[i][j]))\n",
    "\n",
    "\n",
    "# create graph object\n",
    "#G = nx.MultiDiGraph()\n",
    "\n",
    "# nodes correspond to states\n",
    "#G.add_nodes_from(states)\n",
    "#G.add_weighted_edges_from(weigted_edges)\n",
    "\n",
    "#nx.draw(G)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
