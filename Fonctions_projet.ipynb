{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import networkx as nx\n",
    "import community\n",
    "import dask\n",
    "from tqdm import tqdm\n",
    "\n",
    "dask.config.set(scheduler='processes')\n",
    "import glob\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Idea is to remove low eigenvalues that are considered as noise\n",
    "def compute_C_minus_C0(lambdas,v,lambda_plus,removeMarketMode=True): \n",
    "    N=len(lambdas)\n",
    "    C_clean=np.zeros((N, N))\n",
    "    \n",
    "    order = np.argsort(lambdas)\n",
    "    lambdas,v = lambdas[order],v[:,order]\n",
    "    \n",
    "    v_m=np.matrix(v)\n",
    "\n",
    "    # note that the eivenvalues are sorted\n",
    "    for i in range(1*removeMarketMode,N):                            \n",
    "        if lambdas[i]>lambda_plus: \n",
    "            C_clean=C_clean+lambdas[i] * np.dot(v_m[:,i],v_m[:,i].T)  \n",
    "    return C_clean    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LouvainCorrelationClustering(R):  # R is a matrix of return\n",
    "    N = R.shape[1]\n",
    "    T = R.shape[0]\n",
    "\n",
    "    q = N * 1. / T\n",
    "    lambda_plus = (1. + np.sqrt(q)) ** 2\n",
    "\n",
    "    C = R.corr()\n",
    "    lambdas, v = LA.eigh(C)\n",
    "\n",
    "    C_s = compute_C_minus_C0(lambdas, v, lambda_plus)\n",
    "    \n",
    "    mygraph = nx.from_numpy_matrix(np.abs(C_s))\n",
    "    partition = community.community_louvain.best_partition(mygraph, random_state=29)\n",
    "\n",
    "    DF = pd.DataFrame.from_dict(partition, orient=\"index\", columns=['Cluster'])\n",
    "    DF.index = R.columns\n",
    "    return (DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "import os\n",
    "from fastparquet import write, ParquetFile\n",
    "\n",
    "#Check if we already have the clustering or not and compute louvain cluster\n",
    "@dask.delayed\n",
    "def get_Louvain_cluster(R,filename,t_0,t_1, day_state=False) : \n",
    "    \"\"\"\n",
    "    \n",
    "    :param R: \n",
    "    :param filename: \n",
    "    :param t_0: \n",
    "    :param t_1: \n",
    "    :param day_state: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    \n",
    "    # t_0 and t_1 = window\n",
    "    #day_state = TRUE -> market state clustering\n",
    "    \n",
    "    #Check if cluster already exists\n",
    "    if(exists(filename) and os.path.getsize(filename)>0):\n",
    "        \n",
    "        #Import clustering\n",
    "        DF=pd.read_parquet(filename)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        #Do clustering method and saves it\n",
    "        rolling_data=R.iloc[(R.index>=t_0) & (R.index<=t_1)]\n",
    "        \n",
    "        if day_state:\n",
    "            rolling_data=rolling_data.T\n",
    "            \n",
    "        DF= LouvainCorrelationClustering(rolling_data)\n",
    "        DF.to_parquet(filename)\n",
    "        \n",
    "    return(DF)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def RolledCluster(R, cluster_period, filepath, day_state=False, keep_all_info=False, lag_max=20):\n",
    "    \n",
    "    #R = dataframe\n",
    "    #cluster_period = length of rolling window\n",
    "    #day_state = TRUE -> market state clustering\n",
    "    #keep_all_info = TRUE -> rolling window expanding instead of just rolling (pas sur detre utile)\n",
    "    #lag_max = number of clustering\n",
    "    \n",
    "    \n",
    "    #Create list of clusters\n",
    "    liste=[]\n",
    "    \n",
    "    #\n",
    "    if keep_all_info: \n",
    "        if day_state :\n",
    "            for lag in tqdm(range(1,lag_max)):\n",
    "                \n",
    "                #initilisation of window\n",
    "                t_0=R.index[0]\n",
    "                t_1=R.index[0+lag+cluster_period]   \n",
    "                \n",
    "                #compute convenient filename\n",
    "                filename=filepath+\"/{}_{}_cluster_day_keep_all_info.parquet\".format(t_0,t_1)\n",
    "                \n",
    "                #compute cluster\n",
    "                DF= get_Louvain_cluster(R,filename,t_0,t_1, day_state)\n",
    "                \n",
    "                #add cluster to list\n",
    "                liste.append(DF)\n",
    "        else :\n",
    "            for lag in tqdm(range(1,lag_max)):  \n",
    "                \n",
    "                t_0=R.index[0]\n",
    "                t_1=R.index[0+lag+cluster_period]    \n",
    "                \n",
    "                filename=filepath+\"/{}_{}_cluster_keep_all_info.parquet\".format(t_0,t_1)\n",
    "        \n",
    "                DF= get_Louvain_cluster(R,filename,t_0,t_1,day_state)\n",
    "                liste.append(DF)\n",
    "    else : \n",
    "        if day_state :\n",
    "            for lag in tqdm(range(1,lag_max)):  \n",
    "                \n",
    "                t_0=R.index[0+lag]\n",
    "                t_1=R.index[0+lag+cluster_period] \n",
    "                \n",
    "                filename=filepath+\"/{}_{}_cluster_day.parquet\".format(t_0,t_1)\n",
    "                \n",
    "                DF= get_Louvain_cluster(R,filename,t_0,t_1,day_state)\n",
    "                liste.append(DF)\n",
    "        else :\n",
    "            for lag in tqdm(range(1,lag_max)):  \n",
    "                \n",
    "                t_0=R.index[0+lag]\n",
    "                t_1=R.index[0+lag+cluster_period]   \n",
    "                \n",
    "                filename=filepath+\"/{}_{}_cluster.parquet\".format(t_0,t_1)\n",
    "        \n",
    "                DF= get_Louvain_cluster(R,filename,t_0,t_1,day_state)\n",
    "                liste.append(DF)\n",
    "    return(liste)\n",
    "\n",
    "#Renvoie une liste de clusters\n",
    "#Choose the assets (thus N ), choose the calibration length \n",
    "#(T = N/3 is a good choice for market states, T = 3N is a good choice for asset classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the rolling ARI of the clusters\n",
    "from sklearn import metrics\n",
    "\n",
    "def RolledARI(liste):\n",
    "    \n",
    "    #Create list of ARI\n",
    "    ARI=[]\n",
    "    \n",
    "    \n",
    "    for element in tqdm(range(0, len(liste)-1)):\n",
    "        \n",
    "        #Compare consecutive clustering and add measure\n",
    "        new_df=pd.merge(liste[element], liste[element+1], left_index=True, right_index=True)\n",
    "        ARI.append(metrics.adjusted_rand_score(new_df[\"Cluster_x\"], new_df[\"Cluster_y\"]))\n",
    "        \n",
    "        \n",
    "    return(ARI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction qu'on avait fait mais pas sur que ca serve dans notre cas\n",
    "def compute_R(events,tau_max=1000,dtau=1):\n",
    "    taus=range(1,tau_max,dtau)\n",
    "    \n",
    "    R=[-(events[\"mid\"].diff(-tau)*events[\"s\"]).mean() for tau in taus]\n",
    "    \n",
    "    \n",
    "    return np.array(R)\n",
    "\n",
    "def compute_R_h(events):\n",
    "    R=pd.pivot_table(events.groupby(events.index.hour).apply(compute_R).apply(pd.Series), columns=events.groupby(events.index.hour).apply(compute_R).apply(pd.Series).index)\n",
    "    return (R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pour linstant renvoie simplement une matrice de similarite\n",
    "#l'entree (i,j) correspond au pourcentage d'éléments du cluster 2j qui appartiennent aussi au cluster 1i\n",
    "def keep_cluster_number(cluster1,cluster2):\n",
    "    \n",
    "    #rename clusters based on similarity\n",
    "    \n",
    "    #compute number of clusters within each clustering period\n",
    "    n=cluster1[\"Cluster\"].unique().max()\n",
    "    m=cluster2[\"Cluster\"].unique().max()\n",
    "    \n",
    "    #dummy constant to avoid changing twice clusters later\n",
    "    cst=10000\n",
    "    \n",
    "    #Create matrix of similarity\n",
    "    matrix = np.zeros((n+1,m+1))\n",
    "    \n",
    "    for i in range(n+1):\n",
    "        for j in range(m+1):\n",
    "            merged=pd.merge(cluster1,cluster2, left_index=True, right_index=True)\n",
    "            \n",
    "            #get number of days/ticker belonging to cluster x_i and y_j\n",
    "            num= len(merged[(merged[\"Cluster_y\"]==j) & (merged[\"Cluster_x\"]==i)])\n",
    "            \n",
    "            #divide by number of elements in cluster y_j\n",
    "            denom=len(merged[(merged[\"Cluster_y\"]==j)])\n",
    "            \n",
    "            matrix[i][j]= num/denom\n",
    "            #entry (i,j) is the % of elements in y_j coming from x_i\n",
    "    \n",
    "    #get a copy of cluster2 that we change\n",
    "    new_cluster2=cluster2.copy(deep=True) \n",
    "    \n",
    "    #inititialize lists of cluster values\n",
    "    used_values=[]\n",
    "    unrenamed_cluster=[]\n",
    "    \n",
    "    #array with number of element in each cluster\n",
    "    tmp=[]\n",
    "    for i in range(0,m+1):\n",
    "        tmp.append(len(cluster2[cluster2[\"Cluster\"]==i]))\n",
    "\n",
    "    #transform into df\n",
    "    dff=pd.DataFrame(data=tmp)\n",
    "\n",
    "    #sort values by ascending number of element\n",
    "    #relabel cluster based on most important (size) cluster\n",
    "    \n",
    "    dff=dff.sort_values(by=0,ascending=False)\n",
    "\n",
    "    for i in dff.index:\n",
    "        \n",
    "        #get the position of the highest similarity\n",
    "        value= np.argmax(matrix[:,i])\n",
    "    \n",
    "        #verify that this value is not already attributed to another cluster\n",
    "        if not np.isin(value,used_values):\n",
    "        \n",
    "            value_cluster=value\n",
    "        \n",
    "            #update the used values\n",
    "            used_values.append(value)\n",
    "        \n",
    "            #change the new clusters\n",
    "            #add dummy constant to avoid changing twice clusters\n",
    "            new_cluster2[new_cluster2[\"Cluster\"]==i]= (value_cluster+cst)\n",
    "        \n",
    "        else:\n",
    "            #update cluster that need to be allocated a number\n",
    "            unrenamed_cluster.append(i)\n",
    "\n",
    "    #all possible values for cluster number\n",
    "    possible_values= range(0,m+1)\n",
    "\n",
    "    for element in unrenamed_cluster:\n",
    "    \n",
    "        #get smallest unused value\n",
    "        value_cluster= np.min(np.where(~np.isin(possible_values,used_values)))\n",
    "    \n",
    "        #update used values\n",
    "        used_values.append(value)\n",
    "    \n",
    "        #change cluster number\n",
    "        new_cluster2[new_cluster2[\"Cluster\"]==element]= (value_cluster+cst)\n",
    "    \n",
    "    #substract cst to get actual clusters numbers\n",
    "    new_cluster2=new_cluster2-cst\n",
    "    \n",
    "    \n",
    "    return(new_cluster2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One time prediction of cluster belonging\n",
    "#lines = period t\n",
    "#col = period t+1\n",
    "\n",
    "def one_step_pred(cluster):\n",
    "    \n",
    "    #get number of clusters\n",
    "    m=cluster[\"Cluster\"].unique().max()\n",
    "    \n",
    "    #create square matrix\n",
    "    matrix = np.zeros((m+1,m+1))\n",
    "    \n",
    "    for p in range(len(cluster)-1):\n",
    "        i=cluster[\"Cluster\"].iloc[p]\n",
    "        tmp=p+1\n",
    "        j=cluster[\"Cluster\"].iloc[tmp]\n",
    "        \n",
    "        #adds 1 when you go from state i to state j\n",
    "        matrix[i][j]=matrix[i][j]+1\n",
    "        \n",
    "    #divide by total number of changes    \n",
    "    matrix=matrix/(len(cluster)-1)\n",
    "    \n",
    "    #matrix is now a probability matrix\n",
    "    #matrix(i,j) is the probability to go from state i to state j\n",
    "    \n",
    "    return(matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}