{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Vci from cluster\n",
    "def vc_t_i(liste_cluster):\n",
    "    res = []\n",
    "\n",
    "    for l in liste_cluster:\n",
    "        l['values'] = 1\n",
    "        l = l.pivot(columns='Cluster', values='values').fillna(0).sort_index()\n",
    "        res.append(l)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrices(liste_vc1, liste_vc2):\n",
    "    K1 = ((liste_vc1.T @ liste_vc2).T / (liste_vc1.T @ liste_vc1).sum(0)).T\n",
    "\n",
    "    #display((liste_vc2.T @ liste_vc2))\n",
    "    #display((liste_vc2.T @ liste_vc2).sum(0))\n",
    "    #display((liste_vc1.T @ liste_vc2))\n",
    "\n",
    "    K2 = (liste_vc1.T @ liste_vc2) / (liste_vc2.T @ liste_vc2).sum(0)\n",
    "\n",
    "    return K1, K2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def birth(K2):\n",
    "    return list(np.where(K2.sum(0) == 0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def death(K1):\n",
    "    return list(np.where(K1.sum(1) == 0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_growth(K1, theta):\n",
    "    kk = K1.copy()\n",
    "\n",
    "    kk[kk < theta] = 0\n",
    "\n",
    "    kk_ = kk.values\n",
    "    kk_[~(kk_ == kk_.max(axis=1, keepdims=1))] = 0\n",
    "\n",
    "    coordinates = np.argwhere(kk_ > 0)\n",
    "\n",
    "    return [(kk.index[r], kk.columns[c]) for r, c in coordinates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_contraction(K2, theta):\n",
    "    kk = K2.copy()\n",
    "\n",
    "    kk[kk < theta] = 0\n",
    "\n",
    "    kk_ = kk.values\n",
    "    kk_[~(kk_ == kk_.max(axis=0, keepdims=1))] = 0\n",
    "\n",
    "    coordinates = np.argwhere(kk_ > 0)\n",
    "\n",
    "    return [(kk.index[r], kk.columns[c]) for r, c in coordinates]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pickle\n",
    "liste_cluster = pickle.load(open('liste_cluster_cut.p', 'rb'))\n",
    "len(liste_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_clusters(list_clustering):\n",
    "    # compyte a one hot matrix representing the appartenance of the elements to each cluster\n",
    "    vc_list = vc_t_i(list_clustering)\n",
    "\n",
    "    list_clustering[0] = list_clustering[0][['Cluster']]\n",
    "\n",
    "    # This dataframe will serve as a memory for last seen respresentatives of a cluster. It is used in mapping below\n",
    "    repres = pd.DataFrame()\n",
    "    for i in range(len(vc_list) - 1):\n",
    "        repres[vc_list[i].columns] = vc_list[i]\n",
    "\n",
    "        j = i + 1\n",
    "\n",
    "        unmapped_cols = vc_list[j].columns\n",
    "\n",
    "        #compute correlation matrices and growth (row, col) pairs\n",
    "        k1, k2 = correlation_matrices(vc_list[i], vc_list[j])\n",
    "        growth = merge_growth(k1, 0.5)\n",
    "\n",
    "        # create a preliminary dictionary, mapping columns to a list of candidate new names\n",
    "        col_map = dict()\n",
    "        for row, col in growth:\n",
    "            if col not in col_map.keys():\n",
    "                col_map[col] = []\n",
    "            col_map[col].append(row)\n",
    "\n",
    "        # for each candidate list\n",
    "        for k, v in col_map.items():\n",
    "            # if more than one candidate, keep the candidate having the largest representation in the destinantion\n",
    "            if len(v) > 1:\n",
    "                col_map[k] = k2[k].loc[v].idxmax()\n",
    "            # if only one candidate, keep it\n",
    "            else:\n",
    "                col_map[k] = v[0]\n",
    "\n",
    "        # dataframe with available cluster labels and their representatives\n",
    "        available = repres[repres.columns.drop(col_map.values())]\n",
    "\n",
    "        # columns that are not mapped yet, and a dataframe of those columns\n",
    "        unmapped_cols = unmapped_cols.drop(col_map.keys())\n",
    "        unmapped = vc_list[i][unmapped_cols]\n",
    "\n",
    "        # similarity matrix from the point of view of the source\n",
    "        source_similarity = ((available.T @ unmapped).T / (available > 0).sum()).T\n",
    "\n",
    "        # similarity matrix from the point of view of the destination\n",
    "        destination_similarity = (available.T @ unmapped) / (unmapped > 0).sum()\n",
    "\n",
    "        # multiplication of the two above matrices, and keeping only values larger than 0.5*0.5 = 0.25\n",
    "        similarity = source_similarity * destination_similarity\n",
    "        similarity = (similarity >= 0.25) * similarity\n",
    "\n",
    "        # create a mapping using the similarities, greedily selecting the max of the matrix,\n",
    "        # then deleting the mapped row and col\n",
    "        complete = []\n",
    "        while not similarity.empty:\n",
    "            # if the maximum similarity is 0, stop.\n",
    "            if similarity.max().max() == 0:\n",
    "                break\n",
    "\n",
    "            # get the coordinates of the larges value in the matrix\n",
    "            coordinates = np.where(similarity == similarity.max().max())\n",
    "            r = similarity.index[coordinates[0][0]]\n",
    "            c = similarity.index[coordinates[1][0]]\n",
    "\n",
    "            # adding values to our mapping dictionary, then delethe the row and column\n",
    "            col_map[c] = r\n",
    "            similarity.drop(index=r, inplace=True)\n",
    "            similarity.drop(columns=c, inplace=True)\n",
    "\n",
    "        # the remaining unmapped columns are mapped to new labels\n",
    "        max_label = repres.shape[1]\n",
    "        for c in similarity.columns:\n",
    "            if c not in col_map.keys():  # this should be always True\n",
    "                col_map[c] = max_label\n",
    "                max_label += 1\n",
    "\n",
    "        display(col_map)\n",
    "        vc_list[j].rename(columns=col_map, inplace=True)\n",
    "        list_clustering[j] = list_clustering[j][['Cluster']].applymap(col_map.get)\n",
    "\n",
    "        display(vc_list[j])\n",
    "\n",
    "    return list_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}