{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions pour projet\n",
    "\n",
    "need to normalize data ? (cf Dissecting financial markets: Sectors and states) (on devrait utiliser les returns donc pas trop besoin de normaliser)\n",
    "- si on cluster les assets, logiquement la correlation/covariance entre chaque cluster devrait être négative/nulle (parce que la distance serait sans doute la covariance entre les assets), du coup faire une stratégie qui investit dans un représentant (asset ou weigted average) de chaque cluster devrait réduire le risque/volatilité du portfolio (problème étant que on aurait pas un zero-cost portfolio), ou alors prédire le signe de return de chaque cluster à la prochaine période et donc investir en fonction)\n",
    "- si on cluster par jour/période de temps, l'idée serait de prédire le futur état et pour chaque état définir une méthode d'investissement\n",
    "- https://d-nb.info/1108447864/34 papier qui parle de cluster les assets: truc important à noter est que les weights pour chaque cluster sont définis par hierarchical clustering p47-48, et les clusters/weights sont recalculés tous les 130 jours (permet d'éviter de refaire les clusters à chaque fois, et en pls ne suppose plus que les clusters restent les memes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Pour faire des clusters\n",
    "#ce qui doit etre cluster sont les colonnes du dataframe (donc si on veut cluster les dates il faut transposer la matrice)\n",
    "\n",
    "\n",
    "from numpy import linalg as LA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import networkx as nx\n",
    "import community\n",
    "\n",
    "#Idea is to remove low eigenvalues that are considered as noise\n",
    "def compute_C_minus_C0(lambdas,v,lambda_plus,removeMarketMode=True): \n",
    "    N=len(lambdas)\n",
    "    C_clean=np.zeros((N, N))\n",
    "    \n",
    "    order = np.argsort(lambdas)\n",
    "    lambdas,v = lambdas[order],v[:,order]\n",
    "    \n",
    "    v_m=np.matrix(v)\n",
    "\n",
    "    # note that the eivenvalues are sorted\n",
    "    for i in range(1*removeMarketMode,N):                            \n",
    "        if lambdas[i]>lambda_plus: \n",
    "            C_clean=C_clean+lambdas[i] * np.dot(v_m[:,i],v_m[:,i].T)  \n",
    "    return C_clean    \n",
    "    \n",
    "    \n",
    "def LouvainCorrelationClustering(R):   # R is a matrix of return\n",
    "    N=R.shape[1]\n",
    "    T=R.shape[0]\n",
    "\n",
    "    q=N*1./T\n",
    "    lambda_plus=(1.+np.sqrt(q))**2\n",
    "\n",
    "    C=R.corr()\n",
    "    lambdas, v = LA.eigh(C)\n",
    "\n",
    "\n",
    "            \n",
    "    C_s=compute_C_minus_C0(lambdas,v,lambda_plus)\n",
    "    \n",
    "    mygraph= nx.from_numpy_matrix(np.abs(C_s))\n",
    "    partition = community.community_louvain.best_partition(mygraph,random_state=29)\n",
    "\n",
    "    DF=pd.DataFrame.from_dict(partition,orient=\"index\",columns=['Cluster'])\n",
    "    DF.index=R.columns\n",
    "    return(DF)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "peut etre qu'on peut faire une analyse des eigenvalues et eigenvectors (cf cours 9-10)\n",
    "mais la question est qu'on peut pas faire les eigenvalues sur tout le dataset (parce que dans ce cas on analyserait le futur avant meme de faire des clusters par exemple...)\n",
    "\n",
    "De plus, cf cours 11 on pourrait faire varier les méthodes de cluster, que ce soit en changeant la facon de faire le cluster ou de changer le critère qui doit être cluster (return, sign of return, excess return ...)\n",
    "\n",
    "et faudrait aussi du coup faire des investment strategies et les comparer entre elles, et avec un benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creer un rolled clustering \n",
    "#(Suppose que l'index du dataframe est simplement des valeurs et non des dates)-> normalement mtn c bon pour tout\n",
    "#Check si le cluster a pas déjà été fait et sauvegardé\n",
    "#Si non il fait le cluster et le sauvegarde (économie de temps)\n",
    "\n",
    "from os.path import exists\n",
    "import os\n",
    "from fastparquet import write, ParquetFile\n",
    "\n",
    "#R is matrix of return\n",
    "#cluster_period is the period over we cluster\n",
    "#day_state is to know if we have to transpose matrix to cluster periods instead of assets\n",
    "#lag in \"if\" is the number of time we cluster\n",
    "\n",
    "def RolledCluster(R, cluster_period, filepath, day_state=False):\n",
    "    liste=[]\n",
    "    if day_state :\n",
    "        for lag in range(1,20):  #on va devoir changer le range pour le projet mais la c'est pour tester\n",
    "            t_0=R.index[0+lag]\n",
    "            t_1=R.index[0+lag+cluster_period]    \n",
    "            filename=filepath+\"/louvain_cluster/{}_{}_cluster_day.parquet\".format(t_0,t_1)\n",
    "#Check if we already have the clustering or not\n",
    "            if(exists(filename) and os.path.getsize(filename)>0):\n",
    "                DF=pd.read_parquet(filename)\n",
    "            else:\n",
    "                rolling_data=R.iloc[(R.index>=t_0) & (R.index<t_1)]\n",
    "                rolling_data=rolling_data.T\n",
    "                DF= LouvainCorrelationClustering(rolling_data)\n",
    "                DF.to_parquet(filename)\n",
    "            liste.append(DF)\n",
    "    else :\n",
    "        for lag in range(1,20):  #on va devoir changer le range pour le projet mais la c'est pour tester\n",
    "            t_0=R.index[0+lag]\n",
    "            t_1=R.index[0+lag+cluster_period]    \n",
    "            filename=filepath+\"/louvain_cluster/{}_{}_cluster.parquet\".format(t_0,t_1)\n",
    "        \n",
    "            if(exists(filename) and os.path.getsize(filename)>0):\n",
    "                DF=pd.read_parquet(filename)\n",
    "            else:\n",
    "                rolling_data=R.iloc[(R.index>=t_0) & (R.index<t_1)]\n",
    "                DF= LouvainCorrelationClustering(rolling_data)\n",
    "                DF.to_parquet(filename)\n",
    "            liste.append(DF)\n",
    "    return(liste)\n",
    "\n",
    "#Renvoie une liste de clusters\n",
    "#Choose the assets (thus N ), choose the calibration length \n",
    "#(T = N/3 is a good choice for market states, T = 3N is a good choice for asset classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the rolling ARI of the clusters\n",
    "from sklearn import metrics\n",
    "\n",
    "def RolledARI(liste):\n",
    "    ARI=[]\n",
    "    for element in range(0, len(liste)-1):\n",
    "        new_df=pd.merge(liste[element], liste[element+1], left_index=True, right_index=True)\n",
    "        ARI.append(metrics.adjusted_rand_score(new_df[\"Cluster_x\"], new_df[\"Cluster_y\"]))\n",
    "    return(ARI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction qu'on avait fait mais pas sur que ca serve dans notre cas\n",
    "def compute_R(events,tau_max=1000,dtau=1):\n",
    "    taus=range(1,tau_max,dtau)\n",
    "    \n",
    "    ## insert code here\n",
    "    R=[-(events[\"mid\"].diff(-tau)*events[\"s\"]).mean() for tau in taus]\n",
    "    \n",
    "    \n",
    "    return np.array(R)\n",
    "\n",
    "def compute_R_h(events):\n",
    "    R=pd.pivot_table(events.groupby(events.index.hour).apply(compute_R).apply(pd.Series), columns=events.groupby(events.index.hour).apply(compute_R).apply(pd.Series).index)\n",
    "    return (R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pour run les fonctions et les tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "logret= pd.read_parquet(\"/Users/mrobaux/Desktop/Financial big data/data/clean/equities/daily/us-stocks/us_logret.parquet\")\n",
    "\n",
    "\n",
    "new_logret=logret.loc[logret.index>10000]\n",
    "new_logret= new_logret.loc[new_logret.index<16000]\n",
    "dropcol = new_logret.loc[:,new_logret.isna().sum(axis=0)>0].columns\n",
    "new_logret=new_logret.drop(columns=dropcol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "lii= RolledCluster(new_logret,90,\"/Users/mrobaux/Desktop/Financial big data/projet\",True)\n",
    "a=RolledARI(lii)\n",
    "number_of_clusters=[]\n",
    "for element in lii:\n",
    "    number_of_clusters.append(element[\"Cluster\"].unique().max()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename clusters based on similarity\n",
    "#pour linstant renvoie simplement une matrice de similarite\n",
    "#l'entree (i,j) correspond au pourcentage d'éléments du cluster 2j qui appartiennent aussi au cluster 1i\n",
    "def keep_cluster_number(cluster1,cluster2):\n",
    "    n=cluster1[\"Cluster\"].unique().max()\n",
    "    m=cluster2[\"Cluster\"].unique().max()\n",
    "    cst=10000 #pour éviter de changer deux fois les indices plus tard\n",
    "    matrix = np.zeros((n+1,m+1))\n",
    "    for i in range(n+1):\n",
    "        for j in range(m+1):\n",
    "            merged=pd.merge(cluster1,cluster2, left_index=True, right_index=True)\n",
    "            num= len(merged[(merged[\"Cluster_y\"]==j) & (merged[\"Cluster_x\"]==i)])\n",
    "            denom=len(merged[(merged[\"Cluster_y\"]==j)])\n",
    "            matrix[i][j]= num/denom\n",
    "    tmp=cluster2.copy(deep=True) \n",
    "    for j in range(m+1): #Now change cluster number according to similarity\n",
    "        value_cluster= np.where(matrix==np.amax(matrix,0))[1][j]\n",
    "        tmp[tmp[\"Cluster\"]==j]= (value_cluster+cst)\n",
    "    tmp=tmp-cst\n",
    "    return(tmp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One time prediction of cluster belonging\n",
    "#lines = state t\n",
    "#col = state t+1\n",
    "def one_step_pred(cluster):\n",
    "    m=cluster[\"Cluster\"].unique().max()\n",
    "    matrix = np.zeros((m+1,m+1))\n",
    "    for p in range(len(cluster)-1):\n",
    "        i=cluster[\"Cluster\"].iloc[p]\n",
    "        tmp=p+1\n",
    "        j=cluster[\"Cluster\"].iloc[tmp]\n",
    "        matrix[i][j]=matrix[i][j]+1  #Counts number of times when you go from state i to state j\n",
    "    matrix=matrix/(len(cluster)-1)\n",
    "    return(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lii)-1):\n",
    "    lii[i+1]=keep_cluster_number(lii[i],lii[i+1])\n",
    "    matrix = np.zeros((4,4))\n",
    "for cluster in lii:\n",
    "    matrix=matrix+one_step_pred(cluster)\n",
    "matrix=matrix/len(lii)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
